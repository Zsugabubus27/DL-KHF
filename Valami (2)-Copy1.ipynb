{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> It is a copy of the 3rd practical lesson</h1>\n",
    "<h1> I made modifications at the end of this notebook</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYfDp0zoILmf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4dT2-NjILmi"
   },
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Saq0s-XNILmn",
    "outputId": "d4b32541-da5c-4c5a-85ea-0c267723c8ac"
   },
   "outputs": [],
   "source": [
    "def dactivation(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "plt.plot(segedX,dactivation(segedX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZhB41otILmq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    \n",
    "    \n",
    "    def __init__(self, *args):\n",
    "\n",
    "        np.random.seed(123)\n",
    "\n",
    "        self.shape = args\n",
    "        n = len(args)\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(np.ones(self.shape[0]+1))\n",
    "\n",
    "        for i in range(1,n):\n",
    "            self.layers.append(np.ones(self.shape[i]))\n",
    "\n",
    "        self.weights = []\n",
    "        for i in range(n-1):\n",
    "            self.weights.append(np.zeros((self.layers[i].size,\n",
    "                                         self.layers[i+1].size)))\n",
    "\n",
    "        self.dw = [0,]*len(self.weights)\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.weights)):\n",
    " \n",
    "            Z = np.random.random((self.layers[i].size,self.layers[i+1].size))\n",
    "\n",
    "            self.weights[i][...] = (2*Z-1)*1\n",
    "\n",
    "    \n",
    "    def propagate_forward(self, data):\n",
    "\n",
    "        self.layers[0][0:-1] = data\n",
    "\n",
    "        for i in range(1,len(self.shape)):\n",
    "            self.layers[i][...] = activation(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "\n",
    "        return self.layers[-1]\n",
    "\n",
    "    \n",
    "    def propagate_backward(self, target, lrate=0.1):\n",
    "        deltas = []\n",
    "        \n",
    "        error = -(target-self.layers[-1]) # y-y_kalap\n",
    "        \n",
    "        delta = np.multiply(error,dactivation(np.dot(self.layers[-2],self.weights[-1])))\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "        \n",
    "            delta=np.dot(deltas[0],self.weights[i].T)*dactivation(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "            deltas.insert(0,delta)            \n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "        \n",
    "            dw = -lrate*np.dot(layer.T,delta)\n",
    "        \n",
    "            self.weights[i] += dw \n",
    "\n",
    "        \n",
    "            self.dw[i] = dw\n",
    "\n",
    "        \n",
    "        return (error**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGM-FjIyILmu"
   },
   "outputs": [],
   "source": [
    "def learn(network, X, Y, valid_split, test_split, epochs=20, lrate=0.1):\n",
    "\n",
    "\n",
    "        X_train = X[0:int(nb_samples*(1-valid_split-test_split))]\n",
    "        Y_train = Y[0:int(nb_samples*(1-valid_split-test_split))]\n",
    "        X_valid = X[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "        Y_valid = Y[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "        X_test  = X[int(nb_samples*(1-test_split)):]\n",
    "        Y_test  = Y[int(nb_samples*(1-test_split)):]\n",
    "    \n",
    "\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        X_test  = scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "        randperm = np.random.permutation(len(X_train))\n",
    "        X_train, Y_train = X_train[randperm], Y_train[randperm]\n",
    "        \n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_err = 0\n",
    "            for k in range(X_train.shape[0]):\n",
    "                network.propagate_forward( X_train[k] )\n",
    "                train_err += network.propagate_backward( Y_train[k], lrate )\n",
    "            train_err /= X_train.shape[0]\n",
    "\n",
    "\n",
    "            valid_err = 0\n",
    "            o_valid = np.zeros(X_valid.shape[0])\n",
    "            for k in range(X_valid.shape[0]):\n",
    "                o_valid[k] = network.propagate_forward(X_valid[k])\n",
    "                valid_err += (o_valid[k]-Y_valid[k])**2\n",
    "            valid_err /= X_valid.shape[0]\n",
    "\n",
    "            print(\"%d epoch, train_err: %.4f, valid_err: %.4f\" % (i, train_err, valid_err))\n",
    "\n",
    "\n",
    "        print(\"\\n--- TESZTELÃ‰S ---\\n\")\n",
    "        test_err = 0\n",
    "        o_test = np.zeros(X_test.shape[0])\n",
    "        for k in range(X_test.shape[0]):\n",
    "            o_test[k] = network.propagate_forward(X_test[k])\n",
    "            test_err += (o_test[k]-Y_test[k])**2\n",
    "            print(k, X_test[k], '%.2f' % o_test[k], ' (elvart eredmeny: %.2f)' % Y_test[k])\n",
    "        test_err /= X_test.shape[0]\n",
    "\n",
    "        fig1=plt.figure()\n",
    "        plt.scatter(X_test[:,0], X_test[:,1], c=np.round(o_test[:]), cmap=plt.cm.cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bktmacXhILmx"
   },
   "outputs": [],
   "source": [
    "\n",
    "network = MLP(2,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5DdHDQgILmz",
    "outputId": "570f9e4c-e76e-4ce6-fd80-f8c39e53e6a7"
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_samples=1000\n",
    "X = np.zeros((nb_samples,2))\n",
    "Y = np.zeros(nb_samples)\n",
    "for i in range(0,nb_samples,4):\n",
    "    noise = np.random.normal(0,1,8)\n",
    "    X[i], Y[i] = (-2+noise[0],-2+noise[1]), 0\n",
    "    X[i+1], Y[i+1] = (2+noise[2],-2+noise[3]), 1\n",
    "    X[i+2], Y[i+2] = (-2+noise[4],2+noise[5]), 1\n",
    "    X[i+3], Y[i+3] = (2+noise[6],2+noise[7]), 0\n",
    "\n",
    "\n",
    "fig1=plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=Y[:], cmap=plt.cm.cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbhI-rGjILm1",
    "outputId": "5183dcb9-abed-41cf-cfb1-6533ee195b49"
   },
   "outputs": [],
   "source": [
    "\n",
    "network.reset()\n",
    "learn(network, X, Y, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "# HF2 modification by RZ3RVX\n",
    "The previous code snippets are the original scripts for the 3rd course\n",
    "\n",
    "Since utilizing the momentum and l1, l2 optimization methods only affect updating the weights, i am only modifying the back propagation method!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Class that implements Momentum, L1 and L2 regularization\n",
    "# I am inheriting from the already implemented MLP class since i am going to override the propagate_backward function\n",
    "# HF2 start momentum\n",
    "class MLP_momentum(MLP):\n",
    "    def propagate_backward(self, target, lrate=0.1, momentum_factor=0.75):\n",
    "        deltas = []\n",
    "        error = -(target-self.layers[-1])\n",
    "        delta = np.multiply(error,dactivation(np.dot(self.layers[-2],self.weights[-1])))\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        # Calculating gradients in the hidden layers\n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "            delta=np.dot(deltas[0],self.weights[i].T)*dactivation(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "            deltas.insert(0,delta)            \n",
    "        \n",
    "        # --------- Interesting part:\n",
    "        # Updating the weights\n",
    "        for i in range(len(self.weights)):\n",
    "            # Calculating the derivative of the cost function\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "            cost_func_derivate = np.dot(layer.T,delta)\n",
    "            \n",
    "            # deltaW in time t, it depends on deltaW in time (t-1)\n",
    "            deltaW_t = -lrate*cost_func_derivate + momentum_factor*self.dw[i]\n",
    "            # Update weights using deltaW_t\n",
    "            self.weights[i] += deltaW_t\n",
    "            # Store the deltaW\n",
    "            self.dw[i] = deltaW_t\n",
    "        \n",
    "        # Return the error\n",
    "        return (error**2).sum()\n",
    "# HF2 end momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF2 start l1reg\n",
    "class MLP_l1reg(MLP):\n",
    "    def propagate_backward(self, target, lrate=0.1, lambda_1=0.0001):\n",
    "        deltas = []\n",
    "        error = -(target-self.layers[-1])\n",
    "        delta = np.multiply(error,dactivation(np.dot(self.layers[-2],self.weights[-1])))\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        # Calculating gradients in the hidden layers\n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "            delta=np.dot(deltas[0],self.weights[i].T)*dactivation(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "            deltas.insert(0,delta)            \n",
    "        \n",
    "        # --------- Interesting part:\n",
    "        # Updating the weights\n",
    "        for i in range(len(self.weights)):\n",
    "            # Calculating the derivative of the cost function\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "            cost_func_derivate = np.dot(layer.T,delta)\n",
    "            \n",
    "            # For L1 reg the Weights in t+1 equals to:\n",
    "            # Weights(t) - lrate*(derivative_of_weights) - lrate*lambda1*sign(weights)\n",
    "            # As we discussed in the lectures...\n",
    "            \n",
    "            # deltaW in time t\n",
    "            deltaW_t = -lrate*cost_func_derivate - lrate*lambda_1*np.sign(self.weights[i])\n",
    "            # Update weights using deltaW_t\n",
    "            self.weights[i] += deltaW_t\n",
    "            # Store the deltaW\n",
    "            self.dw[i] = deltaW_t\n",
    "        \n",
    "        # Return the error\n",
    "        return (error**2).sum()\n",
    "# HF2 end l1reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF2 start l2reg\n",
    "class MLP_l2reg(MLP):\n",
    "    def propagate_backward(self, target, lrate=0.1, lambda_2=0.001):\n",
    "        deltas = []\n",
    "        error = -(target-self.layers[-1])\n",
    "        delta = np.multiply(error,dactivation(np.dot(self.layers[-2],self.weights[-1])))\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        # Calculating gradients in the hidden layers\n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "            delta=np.dot(deltas[0],self.weights[i].T)*dactivation(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "            deltas.insert(0,delta)            \n",
    "        \n",
    "        # --------- Interesting part:\n",
    "        # Updating the weights\n",
    "        for i in range(len(self.weights)):\n",
    "            # Calculating the derivative of the cost function\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "            cost_func_derivate = np.dot(layer.T,delta)\n",
    "            \n",
    "            # For L2 reg the Weights in t+1 equals to:\n",
    "            # Weights(t) - lrate*(derivative_of_weights) - lrate*lambda2*weights\n",
    "            # As we discussed in the lectures...\n",
    "            \n",
    "            # deltaW in time t\n",
    "            deltaW_t = -lrate*cost_func_derivate - lrate*lambda_2*self.weights[i]\n",
    "            # Update weights using deltaW_t\n",
    "            self.weights[i] += deltaW_t\n",
    "            # Store the deltaW\n",
    "            self.dw[i] = deltaW_t\n",
    "        \n",
    "        # Return the error\n",
    "        return (error**2).sum()\n",
    "# HF2 end l2reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
